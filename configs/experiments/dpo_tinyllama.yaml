# Experiment: DPO fine-tuning with TinyLlama
# Inherits from base.yaml, overrides only what differs.

model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

data:
  source: "Anthropic/hh-rlhf"
  max_samples: 1000

training:
  algorithm: "dpo"
  output_dir: "outputs/dpo_tinyllama"
  num_epochs: 1
  batch_size: 2
  gradient_accumulation_steps: 8
